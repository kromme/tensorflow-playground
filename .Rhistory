devtools::install_github("rstudio/tensorflow")
remove.packages('Rcpp')
install.packages('Rcpp')
install.packages('C:/Users/j.kromme/Documents/rcpp/Rcpp', repos = NULL, type="source")
install.packages('reticulate')
devtools::install_github("rstudio/reticulate")
devtools::install_github("rstudio/tensorflow")
library(tensorflow)
x_data <- runif(100, min=0, max=1)
y_data <- x_data * 0.1 + 0.3
W <- tf$Variable(tf$random_uniform(shape(1L), -1.0, 1.0))
b <- tf$Variable(tf$zeros(shape(1L)))
y <- W * x_data + b
detach(tensorflow)
detach("package:vegan", unload=TRUE)
detach("package:tensorflow", unload=TRUE)
library(reticulate)
use_condaenv("snakes")
library(tensorflow)
library(reticulate)
use_condaenv("snakes")
library(tensorflow)
x_data <- runif(100, min=0, max=1)
y_data <- x_data * 0.1 + 0.3
W <- tf$Variable(tf$random_uniform(shape(1L), -1.0, 1.0))
b <- tf$Variable(tf$zeros(shape(1L)))
y <- W * x_data + b
loss <- tf$reduce_mean((y - y_data) ^ 2)
optimizer <- tf$train$GradientDescentOptimizer(0.5)
train <- optimizer$minimize(loss)
sess = tf$Session()
sess$run(tf$global_variables_initializer())
for (step in 1:201) {
sess$run(train)
if (step %% 20 == 0)
cat(step, "-", sess$run(W), sess$run(b), "\n")
}
rm(list = ls())
datasets <- tf$contrib$learn$datasets
mnist <- datasets$mnist$read_data_sets("MNIST-data", one_hot = TRUE)
head(mnist$train$images)
image(mnist$train$images[1])
image(as.matrix(mnist$train$images[1]))
dim(as.matrix(mnist$train$images[1]))
dim(mnist$train$images[1])
str(mnist$train$images[1])
head(mnist$train$images[1])
head(mnist$train$images[1, 1:17])
head(mnist$train$images[1, 1:27])
as.matrix(mnist$train$images[1, 1:27])
str(mnist$train$images)
matrix(mnist$train$images, nrow = 27)
image(matrix(mnist$train$images[1:784], nrow = 27))
image(matrix(mnist$train$images[1:784], nrow = 28))
matrix(mnist$train$images[1:784], nrow = 28)
i = 1
start_i = i * 784
end_i = ((i+2) * 784) - 1
i = 2
start_i = i * 784
end_i = ((i+2) * 784) - 1
start_i = (i-1) + 784
start_i = (i-1) + 783
end_i = ((i+2) * 784) - 1
end_i = start_i +784
matrix(mnist$train$images[1:784], nrow = 28)
image(matrix(mnist$train$images[1:784], nrow = 28))
image(matrix(mnist$train$images[start_i, end_i], nrow = 28))
image(matrix(mnist$train$images[start_i: end_i], nrow = 28))
length(start_i:end_i)
i = 2
start_i = (i-1) + 784
end_i = start_i +784
image(matrix(mnist$train$images[start_i: end_i], nrow = 28))
length(start_i:end_i)
end_i = start_i +783
image(matrix(mnist$train$images[start_i: end_i], nrow = 28))
summary(matrix(mnist$train$images[start_i: end_i], nrow = 28))
i = 200
start_i = (i-1) + 784
end_i = start_i +783
image(matrix(mnist$train$images[start_i: end_i], nrow = 28))
trainset_df <- mnist$train$images
dim(trainset_df)
trainset_df[1,]
matrix(trainset_df, ncol = 28)
image(matrix(trainset_df[i,], ncol = 28))
str(mnist$train)
y_df <- minst$train$label
y_df <- mnist$train$label
y_df <- mnist$train$labels
dim(y_df)
y_df[i,]
0:9[y_df[i,]==1]
y_df[i,]==1
0:9
0:9 [y_df[i,]==1]
x = 0:9
x[y_df[i,]==1]
x <- tf$placeholder(tf$float32, shape(NULL, 784L))
W <- tf$Variable(tf$zeros(shape(784L, 10L)))
b <- tf$Variable(tf$zeros(shape(10L)))
y <- tf$nn$softmax(tf$matmul(x, W) + b)
y_ <- tf$placeholder(tf$float32, shape(NULL, 10L))
train_step <- tf$train$GradientDescentOptimizer(0.5)$minimize(cross_entropy)
cross_entropy <- tf$reduce_mean(-tf$reduce_sum(y_ * log(y), reduction_indices=1L))
train_step <- tf$train$GradientDescentOptimizer(0.5)$minimize(cross_entropy)
optimizer <- tf$train$GradientDescentOptimizer(0.5)
train_step <- optimizer$minimize(cross_entropy)
init = tf$global_variables_initializer()
sess <- tf$Session()
sess$run(init)
for (i in 1:1000) {
batches <- mnist$train$next_batch(100L)
batch_xs <- batches[[1]]
batch_ys <- batches[[2]]
sess$run(train_step,
feed_dict = dict(x = batch_xs, y_ = batch_ys))
}
correct_prediction <- tf$equal(tf$argmax(y, 1L), tf$argmax(y_, 1L))
accuracy <- tf$reduce_mean(tf$cast(correct_prediction, tf$float32))
sess$run(accuracy,
feed_dict = dict(x = mnist$test$images, y_ = mnist$test$labels))
rm(list = ls())
# first reshape x to a 4d tensor, with the second and third dimensions corresponding to image width and height, and the final dimension corresponding to the number of color channel
library(reticulate)
use_condaenv("snakes")
library(tensorflow)
#  ------ Load mnist data
# The MNIST data is split into three parts: 55,000 data points of training data (mnist$train), 10,000 points of test data (mnist$test), and 5,000 points of validation data (mnist$validation).
#  we’re going to want our labels as “one-hot vectors”. A one-hot vector is a vector which is 0 in most dimensions, and 1 in a single dimension. In this case, the nth digit will be represented as a vector which is 1 in the nth dimension. For example, 3 would be [0,0,0,1,0,0,0,0,0,0].
datasets <- tf$contrib$learn$datasets
mnist <- datasets$mnist$read_data_sets("MNIST-data", one_hot = TRUE)
sess <- tf$InteractiveSession()
X <- tf$placeholder(tf$float32, shape(NULL, 784L))
y_ <- tf$placeholder(tf$float32, shape(NULL, 10L))
W <- tf$Variable(tf$zeros(shape(784L, 10L)))
b <- tf$Variable(tf$zeros(shape(10L)))
sess$run(tf$global_variables_initializer())
y <- tf$nn$softmax(tf$matmul(x, W) + b)
y <- tf$nn$softmax(tf$matmul(X, W) + b)
# define cost function
# Cross-entropy arises from thinking about information compressing codes in information theory
# H_y`(y) = - sum_i(y_i` * log (y_i))
cross_entropy <- tf$reduce_mean(-tf$reduce_sum(y_ * log(y), reduction_indices=1L))
# it can automatically use the backpropagation algorithm
optimizer <- tf$train$GradientDescentOptimizer(0.5)
train_step <- optimizer$minimize(cross_entropy)
init = tf$global_variables_initializer()
correct_prediction <- tf$equal(tf$argmax(y, 1L), tf$argmax(y_, 1L))
accuracy <- tf$reduce_mean(tf$cast(correct_prediction, tf$float32))
sess$run(accuracy,
feed_dict = dict(x = mnist$test$images, y_ = mnist$test$labels))
#  initialize weights with a small amount of noise for symmetry breaking, and to prevent 0 gradients
weight_variable <- function(shape) {
initial <- tf$truncated_normal(shape, stddev=0.1)
tf$Variable(initial)
}
bias_variable <- function(shape) {
initial <- tf$constant(0.1, shape=shape)
tf$Variable(initial)
}
# Our convolutions uses a stride of one and are zero padded so that the output is the same size as the input
# Our pooling is plain old max pooling over 2x2 blocks.
conv2d <- function(x, W) {
tf$nn$conv2d(x, W, strides=c(1L, 1L, 1L, 1L), padding='SAME')
}
max_pool_2x2 <- function(x) {
tf$nn$max_pool(
x,
ksize=c(1L, 2L, 2L, 1L),
strides=c(1L, 2L, 2L, 1L),
padding='SAME')
}
# -- first conv layer
# It will consist of convolution, followed by max pooling. The convolutional will compute 32 features for each 5x5 patch
# Its weight tensor will have a shape of (5, 5, 1, 32). The first two dimensions are the patch size, the next is the number
# of input channels, and the last is the number of output channels. We will also have a bias vector with a component for each output channel.
W_conv1 <- weight_variable(shape(5L, 5L, 1L, 32L))
b_conv1 <- bias_variable(shape(32L))
x_image <- tf$reshape(x, shape(-1L, 28L, 28L, 1L))
rm(list = ls())
datasets <- tf$contrib$learn$datasets
mnist <- datasets$mnist$read_data_sets("MNIST-data", one_hot = TRUE)
sess <- tf$Session()
X <- tf$placeholder(tf$float32, shape(NULL, 784L))
y_ <- tf$placeholder(tf$float32, shape(NULL, 10L))
W <- tf$Variable(tf$zeros(shape(784L, 10L)))
b <- tf$Variable(tf$zeros(shape(10L)))
sess$run(tf$global_variables_initializer())
y <- tf$nn$softmax(tf$matmul(X, W) + b)
cross_entropy <- tf$reduce_mean(-tf$reduce_sum(y_ * log(y), reduction_indices=1L))
optimizer <- tf$train$GradientDescentOptimizer(0.5)
train_step <- optimizer$minimize(cross_entropy)
weight_variable <- function(shape) {
initial <- tf$truncated_normal(shape, stddev=0.1)
tf$Variable(initial)
}
bias_variable <- function(shape) {
initial <- tf$constant(0.1, shape=shape)
tf$Variable(initial)
}
conv2d <- function(x, W) {
tf$nn$conv2d(x, W, strides=c(1L, 1L, 1L, 1L), padding='SAME')
}
max_pool_2x2 <- function(x) {
tf$nn$max_pool(
x,
ksize=c(1L, 2L, 2L, 1L),
strides=c(1L, 2L, 2L, 1L),
padding='SAME')
}
W_conv1 <- weight_variable(shape(5L, 5L, 1L, 32L))
b_conv1 <- bias_variable(shape(32L))
x_image <- tf$reshape(x, shape(-1L, 28L, 28L, 1L))
init = tf$global_variables_initializer()
sess$run(init)
x_image <- tf$reshape(x, shape(-1L, 28L, 28L, 1L))
