{
    "collab_server" : "",
    "contents" : "rm(list = ls())\nlibrary(reticulate)\nuse_condaenv(\"snakes\")\n\nlibrary(tensorflow)\n\n#  ------ Load mnist data\n# The MNIST data is split into three parts: 55,000 data points of training data (mnist$train), 10,000 points of test data (mnist$test), and 5,000 points of validation data (mnist$validation). \n#  we’re going to want our labels as “one-hot vectors”. A one-hot vector is a vector which is 0 in most dimensions, and 1 in a single dimension. In this case, the nth digit will be represented as a vector which is 1 in the nth dimension. For example, 3 would be [0,0,0,1,0,0,0,0,0,0]. \ndatasets <- tf$contrib$learn$datasets\nmnist <- datasets$mnist$read_data_sets(\"MNIST-data\", one_hot = TRUE)\n\ntrainset_df <- mnist$train$images\ndim(trainset_df)\n\ny_df <- mnist$train$labels\ndim(y_df)\n\n# visualize\nx = 0:9\ni = 1\nimage(matrix(trainset_df[i,], ncol = 28))\nx[y_df[i,]==1]\n\n\n\n\n\n\n# ------ Theory\n# . If you want to assign probabilities to an object being one of several different things, softmax is the thing to do, \n# because softmax gives us a list of values between 0 and 1 that add up to 1. Even later on, when we train more sophisticated models, \n# the final step will be a layer of softmax.\n# first we add up the evidence of our input being in certain classes, and then we convert that evidence into probabilities.\n\n# We also add some extra evidence called a bias. Basically, we want to be able to say that some things are more likely \n# independent of the input. The result is that the evidence for a class i given an input x is:\n# evidence = sum_j(W_i,j * X_j + B_i), where W = weights, b = bias, for class i and j is an index for summing over the pixels.\n# y = softmax(evidence) = normalize(exp(evidence)). softmax_i = (exp(X_i) / sum_j(exp(X_j)))\n\n\n\n# ------ Create the model\n# TensorFlow also does its heavy lifting outside R, but it takes things a step further to avoid this overhead. Instead of running a single expensive operation independently from R, TensorFlow lets us describe a graph of interacting operations that run entirely outside R (Approaches like this can be seen in a few machine learning libraries.)\n\n# We describe these interacting operations by manipulating symbolic variables. Let’s create one (to access the TensorFlow API we reference the tf object that is exported by the tensorflow package)\n# x isn’t a specific value. It’s a placeholder, a value that we’ll input when we ask TensorFlow to run a computation\n# NULL means that it can be any size\n\n\nx <- tf$placeholder(tf$float32, shape(NULL, 784L))\n\n#  A Variable is a modifiable tensor that lives in TensorFlow’s graph of interacting operations.\nW <- tf$Variable(tf$zeros(shape(784L, 10L)))\nb <- tf$Variable(tf$zeros(shape(10L)))\n\n# First, we multiply x by W with the expression tf$matmul(x, W). This is flipped from when we multiplied them in our equation, where we had Wx, as a small trick to deal with x being a 2D tensor with multiple inputs. We then add b, and finally apply tf$nn$softmax\ny <- tf$nn$softmax(tf$matmul(x, W) + b)\n\n# --- Define loss and optimizer\n# new placeholder for predictions\ny_ <- tf$placeholder(tf$float32, shape(NULL, 10L))\n\n\n# define cost function\n# Cross-entropy arises from thinking about information compressing codes in information theory\n# H_y`(y) = - sum_i(y_i` * log (y_i))\ncross_entropy <- tf$reduce_mean(-tf$reduce_sum(y_ * log(y), reduction_indices=1L))\n\n# it can automatically use the backpropagation algorithm\noptimizer <- tf$train$GradientDescentOptimizer(0.5)\ntrain_step <- optimizer$minimize(cross_entropy)\n\n# we have to create an operation to initialize the variables we created. Note that this defines the operation but does not run it yet\ninit = tf$global_variables_initializer()\n\n# Create session and initialize  variables\nsess <- tf$Session()\nsess$run(init)\n\n\n\n# Train: we’ll run the training step 1000 times!\n# Each step of the loop, we get a “batch” of one hundred random data points from our training set. We run train_step feeding in the batches data to replace the placeholders.\n\n# Using small batches of random data is called stochastic training – in this case, stochastic gradient descent. \n# Ideally, we’d like to use all our data for every step of training because that would give us a better sense of what we should be doing, but that’s expensive. \n# So, instead, we use a different subset every time. Doing this is cheap and has much of the same benefit.\nfor (i in 1:1000) {\n  batches <- mnist$train$next_batch(100L)\n  batch_xs <- batches[[1]]\n  batch_ys <- batches[[2]]\n  sess$run(train_step,\n           feed_dict = dict(x = batch_xs, y_ = batch_ys))\n}\n\n# Test trained model\ncorrect_prediction <- tf$equal(tf$argmax(y, 1L), tf$argmax(y_, 1L))\naccuracy <- tf$reduce_mean(tf$cast(correct_prediction, tf$float32))\nsess$run(accuracy,\n         feed_dict = dict(x = mnist$test$images, y_ = mnist$test$labels))\n\n\n\n\n",
    "created" : 1489939413073.000,
    "dirty" : true,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3675165197",
    "id" : "5AEC7204",
    "lastKnownWriteTime" : 1490028638,
    "last_content_update" : 1491120902866,
    "path" : "C:/Users/j.kromme/Desktop/sandbox/tensorflow-playground/02 simple MNIST tutorial.R",
    "project_path" : "02 simple MNIST tutorial.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 3,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}