{
    "collab_server" : "",
    "contents" : "rm(list = ls())\n\n# first reshape x to a 4d tensor, with the second and third dimensions corresponding to image width and height, and the final dimension corresponding to the number of color channel\nlibrary(reticulate)\nuse_condaenv(\"snakes\")\n\nlibrary(tensorflow)\n\n#  ------ Load mnist data\n# The MNIST data is split into three parts: 55,000 data points of training data (mnist$train), 10,000 points of test data (mnist$test), and 5,000 points of validation data (mnist$validation). \n#  we’re going to want our labels as “one-hot vectors”. A one-hot vector is a vector which is 0 in most dimensions, and 1 in a single dimension. In this case, the nth digit will be represented as a vector which is 1 in the nth dimension. For example, 3 would be [0,0,0,1,0,0,0,0,0,0]. \ndatasets <- tf$contrib$learn$datasets\nmnist <- datasets$mnist$read_data_sets(\"MNIST-data\", one_hot = TRUE)\n\n\n\nsess <- tf$InteractiveSession()\n\n\nx <- tf$placeholder(tf$float32, shape(NULL, 784L))\ny_ <- tf$placeholder(tf$float32, shape(NULL, 10L))\n\n\n\n#  A Variable is a modifiable tensor that lives in TensorFlow’s graph of interacting operations.\nW <- tf$Variable(tf$zeros(shape(784L, 10L)))\nb <- tf$Variable(tf$zeros(shape(10L)))\n\nsess$run(tf$global_variables_initializer())\n\n# First, we multiply x by W with the expression tf$matmul(x, W). This is flipped from when we multiplied them in our equation, where we had Wx, as a small trick to deal with x being a 2D tensor with multiple inputs. We then add b, and finally apply tf$nn$softmax\ny <- tf$nn$softmax(tf$matmul(x, W) + b)\n\n# --- Define loss and optimizer\n# new placeholder for predictions\n\n\n# define cost function\n# Cross-entropy arises from thinking about information compressing codes in information theory\n# H_y`(y) = - sum_i(y_i` * log (y_i))\ncross_entropy <- tf$reduce_mean(-tf$reduce_sum(y_ * log(y), reduction_indices=1L))\n\n# it can automatically use the backpropagation algorithm\noptimizer <- tf$train$GradientDescentOptimizer(0.5)\ntrain_step <- optimizer$minimize(cross_entropy)\n\n# we have to create an operation to initialize the variables we created. Note that this defines the operation but does not run it yet\ninit = tf$global_variables_initializer()\n\n# Create session and initialize  variables\nsess <- tf$Session()\nsess$run(init)\n\n\n\n# Train: we’ll run the training step 1000 times!\n# Each step of the loop, we get a “batch” of one hundred random data points from our training set. We run train_step feeding in the batches data to replace the placeholders.\n\n# Using small batches of random data is called stochastic training – in this case, stochastic gradient descent. \n# Ideally, we’d like to use all our data for every step of training because that would give us a better sense of what we should be doing, but that’s expensive. \n# So, instead, we use a different subset every time. Doing this is cheap and has much of the same benefit.\nfor (i in 1:1000) {\n  batches <- mnist$train$next_batch(100L)\n  batch_xs <- batches[[1]]\n  batch_ys <- batches[[2]]\n  sess$run(train_step,\n           feed_dict = dict(x = batch_xs, y_ = batch_ys))\n}\n\n# Test trained model\ncorrect_prediction <- tf$equal(tf$argmax(y, 1L), tf$argmax(y_, 1L))\naccuracy <- tf$reduce_mean(tf$cast(correct_prediction, tf$float32))\nsess$run(accuracy,\n         feed_dict = dict(x = mnist$test$images, y_ = mnist$test$labels))\n\n\n\n\n#  initialize weights with a small amount of noise for symmetry breaking, and to prevent 0 gradients\n\n\nweight_variable <- function(shape) {\n  initial <- tf$truncated_normal(shape, stddev=0.1)\n  tf$Variable(initial)\n}\n\nbias_variable <- function(shape) {\n  initial <- tf$constant(0.1, shape=shape)\n  tf$Variable(initial)\n}\n\n# Our convolutions uses a stride of one and are zero padded so that the output is the same size as the input\n# Our pooling is plain old max pooling over 2x2 blocks.\n\nconv2d <- function(x, W) {\n  tf$nn$conv2d(x, W, strides=c(1L, 1L, 1L, 1L), padding='SAME')\n}\n\nmax_pool_2x2 <- function(x) {\n  tf$nn$max_pool(\n    x, \n    ksize=c(1L, 2L, 2L, 1L),\n    strides=c(1L, 2L, 2L, 1L), \n    padding='SAME')\n}\n\n\n# -- first conv layer\n# It will consist of convolution, followed by max pooling. The convolutional will compute 32 features for each 5x5 patch\n# Its weight tensor will have a shape of (5, 5, 1, 32). The first two dimensions are the patch size, the next is the number\n# of input channels, and the last is the number of output channels. We will also have a bias vector with a component for each output channel.\n\nW_conv1 <- weight_variable(shape(5L, 5L, 1L, 32L))\nb_conv1 <- bias_variable(shape(32L))\n\n# first reshape x to a 4d tensor, with the second and third dimensions corresponding to image width and height, and the final dimension corresponding to the number of color channel\nx_image <- tf$reshape(x, shape(-1L, 28L, 28L, 1L))\n\n\n\nh_conv1 <- tf$nn$relu(conv2d(x_image, W_conv1) + b_conv1)\nh_pool1 <- max_pool_2x2(h_conv1)\n\n\n\nW_conv2 <- weight_variable(shape = shape(5L, 5L, 32L, 64L))\nb_conv2 <- bias_variable(shape = shape(64L))\n\nh_conv2 <- tf$nn$relu(conv2d(h_pool1, W_conv2) + b_conv2)\nh_pool2 <- max_pool_2x2(h_conv2)\n\nW_fc1 <- weight_variable(shape(7L * 7L * 64L, 1024L))\nb_fc1 <- bias_variable(shape(1024L))\n\nh_pool2_flat <- tf$reshape(h_pool2, shape(-1L, 7L * 7L * 64L))\nh_fc1 <- tf$nn$relu(tf$matmul(h_pool2_flat, W_fc1) + b_fc1)\n\n# --- Dropout\n# To reduce overfitting, we will apply dropout before the readout layer. We create a placeholder for the probability that a neuron’s output is kept during dropout. This allows us to turn dropout on during training, and turn it off during testing. TensorFlow’s tf$nn$dropout op automatically handles scaling neuron outputs in addition to masking them, so dropout just works without any additional scaling.1\n\nkeep_prob <- tf$placeholder(tf$float32)\nh_fc1_drop <- tf$nn$dropout(h_fc1, keep_prob)\n\n# --- Readout Layer\n# we add a softmax layer, just like for the one layer softmax regression above.\n\nW_fc2 <- weight_variable(shape(1024L, 10L))\nb_fc2 <- bias_variable(shape(10L))\n\ny_conv <- tf$nn$softmax(tf$matmul(h_fc1_drop, W_fc2) + b_fc2)\n\n# --- Train and Evaluate the Model\n\n# How well does this model do? To train and evaluate it we will use code that is nearly identical to that for the simple one layer SoftMax network above.\n# The differences are that:\n# We will replace the steepest gradient descent optimizer with the more sophisticated ADAM optimizer.\n# We will include the additional parameter keep_prob in feed_dict to control the dropout rate.\n# We will add logging to every 100th iteration in the training process.\n# Feel free to go ahead and run this code, but it does 20,000 training iterations and may take a while (possibly up to half an hour), depending on your processor.\n\ncross_entropy <- tf$reduce_mean(-tf$reduce_sum(y_ * tf$log(y_conv), reduction_indices=1L))\ntrain_step <- tf$train$AdamOptimizer(1e-4)$minimize(cross_entropy)\ncorrect_prediction <- tf$equal(tf$argmax(y_conv, 1L), tf$argmax(y_, 1L))\naccuracy <- tf$reduce_mean(tf$cast(correct_prediction, tf$float32))\n\nsess <- tf$InteractiveSession()\nsess$run(tf$global_variables_initializer())\n\n\nfor (i in 1:10000) {\n  batch <- mnist$train$next_batch(50L)\n  if (i %% 100 == 0) {\n  \n      train_accuracy <- accuracy$eval(feed_dict = dict(\n      x = batch[[1]], y_ = batch[[2]], keep_prob = 1.0))\n    cat(sprintf(\"step %d, training accuracy %g\\n\", i, train_accuracy))\n  }\n  train_step$run(feed_dict = dict(\n    x = batch[[1]], y_ = batch[[2]], keep_prob = 0.5))\n}\n\ntrain_accuracy <- accuracy$eval(feed_dict = dict(\n  x = mnist$test$images, y_ = mnist$test$labels, keep_prob = 1.0))\ncat(sprintf(\"test accuracy %g\", train_accuracy))\n",
    "created" : 1490347306866.000,
    "dirty" : true,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3783964552",
    "id" : "DF36352D",
    "lastKnownWriteTime" : 1491124115,
    "last_content_update" : 1491223427466,
    "path" : "C:/Users/j.kromme/Desktop/sandbox/tensorflow-playground/03 multilayer ConvNet MNIST tutorial.R",
    "project_path" : "03 multilayer ConvNet MNIST tutorial.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 4,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}